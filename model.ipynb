{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import r2_score, fbeta_score, mean_squared_error, accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split, KFold, StratifiedKFold\n",
    "import xgboost\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('X_y.npz')\n",
    "X, y = data['X'], data['y']\n",
    "X_other, X_test, y_other, y_test = train_test_split(X,y,test_size = 0.2,random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KFold in module sklearn.model_selection._split:\n",
      "\n",
      "class KFold(_UnsupportedGroupCVMixin, _BaseKFold)\n",
      " |  KFold(n_splits=5, *, shuffle=False, random_state=None)\n",
      " |\n",
      " |  K-Fold cross-validator.\n",
      " |\n",
      " |  Provides train/test indices to split data in train/test sets. Split\n",
      " |  dataset into k consecutive folds (without shuffling by default).\n",
      " |\n",
      " |  Each fold is then used once as a validation while the k - 1 remaining\n",
      " |  folds form the training set.\n",
      " |\n",
      " |  Read more in the :ref:`User Guide <k_fold>`.\n",
      " |\n",
      " |  For visualisation of cross-validation behaviour and\n",
      " |  comparison between common scikit-learn split methods\n",
      " |  refer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`\n",
      " |\n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_splits : int, default=5\n",
      " |      Number of folds. Must be at least 2.\n",
      " |\n",
      " |      .. versionchanged:: 0.22\n",
      " |          ``n_splits`` default value changed from 3 to 5.\n",
      " |\n",
      " |  shuffle : bool, default=False\n",
      " |      Whether to shuffle the data before splitting into batches.\n",
      " |      Note that the samples within each split will not be shuffled.\n",
      " |\n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      When `shuffle` is True, `random_state` affects the ordering of the\n",
      " |      indices, which controls the randomness of each fold. Otherwise, this\n",
      " |      parameter has no effect.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |\n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.model_selection import KFold\n",
      " |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      " |  >>> y = np.array([1, 2, 3, 4])\n",
      " |  >>> kf = KFold(n_splits=2)\n",
      " |  >>> kf.get_n_splits(X)\n",
      " |  2\n",
      " |  >>> print(kf)\n",
      " |  KFold(n_splits=2, random_state=None, shuffle=False)\n",
      " |  >>> for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
      " |  ...     print(f\"Fold {i}:\")\n",
      " |  ...     print(f\"  Train: index={train_index}\")\n",
      " |  ...     print(f\"  Test:  index={test_index}\")\n",
      " |  Fold 0:\n",
      " |    Train: index=[2 3]\n",
      " |    Test:  index=[0 1]\n",
      " |  Fold 1:\n",
      " |    Train: index=[0 1]\n",
      " |    Test:  index=[2 3]\n",
      " |\n",
      " |  Notes\n",
      " |  -----\n",
      " |  The first ``n_samples % n_splits`` folds have size\n",
      " |  ``n_samples // n_splits + 1``, other folds have size\n",
      " |  ``n_samples // n_splits``, where ``n_samples`` is the number of samples.\n",
      " |\n",
      " |  Randomized CV splitters may return different results for each call of\n",
      " |  split. You can make the results identical by setting `random_state`\n",
      " |  to an integer.\n",
      " |\n",
      " |  See Also\n",
      " |  --------\n",
      " |  StratifiedKFold : Takes class information into account to avoid building\n",
      " |      folds with imbalanced class distributions (for binary or multiclass\n",
      " |      classification tasks).\n",
      " |\n",
      " |  GroupKFold : K-fold iterator variant with non-overlapping groups.\n",
      " |\n",
      " |  RepeatedKFold : Repeats K-Fold n times.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      KFold\n",
      " |      _UnsupportedGroupCVMixin\n",
      " |      _BaseKFold\n",
      " |      BaseCrossValidator\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, n_splits=5, *, shuffle=False, random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _UnsupportedGroupCVMixin:\n",
      " |\n",
      " |  split(self, X, y=None, groups=None)\n",
      " |      Generate indices to split data into training and test set.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Training data, where `n_samples` is the number of samples\n",
      " |          and `n_features` is the number of features.\n",
      " |\n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          The target variable for supervised learning problems.\n",
      " |\n",
      " |      groups : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |\n",
      " |      Yields\n",
      " |      ------\n",
      " |      train : ndarray\n",
      " |          The training set indices for that split.\n",
      " |\n",
      " |      test : ndarray\n",
      " |          The testing set indices for that split.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _UnsupportedGroupCVMixin:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseKFold:\n",
      " |\n",
      " |  get_n_splits(self, X=None, y=None, groups=None)\n",
      " |      Returns the number of splitting iterations in the cross-validator.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |\n",
      " |      y : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |\n",
      " |      groups : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      n_splits : int\n",
      " |          Returns the number of splitting iterations in the cross-validator.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseCrossValidator:\n",
      " |\n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |\n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |\n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |\n",
      " |  __init_subclass__(**kwargs)\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |\n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |\n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |\n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(KFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_model(X_other,y_other, X_test, y_test, random_state, verbose=1):\n",
    "    # make into row vectors to avoid an obnoxious sklearn/xgb warning\n",
    "    n_splits = 5\n",
    "    this_cv = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=random_state)\n",
    "    y_other = np.reshape(np.array(y_other), (1, -1)).ravel()\n",
    "    y_test = np.reshape(np.array(y_test), (1, -1)).ravel()\n",
    "\n",
    "    clf = xgboost.XGBClassifier(n_jobs=-1,random_state=random_state)\n",
    "    \n",
    "    # find the best parameter set\n",
    "    param_grid = {\"learning_rate\": [0.03],\n",
    "                  \"n_estimators\": [100],\n",
    "                  \"seed\": [0],\n",
    "                  \"reg_alpha\": [0e0, 1e-2, 1e-1, 1e0, 1e1, 1e2],\n",
    "                  \"reg_lambda\": [0e0, 1e-2, 1e-1, 1e0, 1e1, 1e2],\n",
    "                  \"missing\": [np.nan], \n",
    "                  #\"max_depth\": [1,3,10,30,100,],\n",
    "                  \"colsample_bytree\": [0.9],              \n",
    "                  \"subsample\": [0.66]}\n",
    "\n",
    "    pg = ParameterGrid(param_grid)\n",
    "\n",
    "    scores = np.zeros((len(pg),n_splits))\n",
    "\n",
    "    for i in range(len(pg)):\n",
    "        if verbose >= 5:\n",
    "                print(\"Param set \" + str(i + 1) + \" / \" + str(len(pg)))\n",
    "        params = pg[i]\n",
    "        for j, (train_index, val_index) in enumerate(this_cv.split(X_other,y_other)):\n",
    "            X_train = X_other.iloc[train_index,:]\n",
    "            y_train = y_other[train_index]\n",
    "            X_val = X_other.iloc[val_index,:]\n",
    "            y_val = y_other[val_index]\n",
    "            clf.set_params(**params,early_stopping_rounds=50)\n",
    "            eval_set = [(X_val, y_val)]\n",
    "            clf.fit(X_train, y_train,\n",
    "                    eval_set=eval_set, verbose=False)# with early stopping\n",
    "            y_val_pred = clf.predict(X_val)\n",
    "            scores[i,j] = accuracy_score(y_val,y_val_pred)\n",
    "    scores = np.mean(scores, axis=1)\n",
    "\n",
    "    best_params = np.array(pg)[scores == np.max(scores)]\n",
    "    if verbose >= 4:\n",
    "        print('Test set max score and best parameters are:')\n",
    "        print(np.max(scores))\n",
    "        print(best_params)\n",
    "\n",
    "    # test the model on the test set with best parameter set\n",
    "    clf.set_params(**best_params[0],early_stopping_rounds=50)\n",
    "    clf.fit(X_train,y_train,\n",
    "            eval_set=eval_set, verbose=False)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print ('The accuracy is:',accuracy_score(y_test,y_test_pred))\n",
    "    if verbose >= 2:\n",
    "        print ('The predictions are:')\n",
    "        print (y_test_pred)\n",
    "    if verbose >= 3:\n",
    "        print(\"Feature importances:\")\n",
    "        print(clf.feature_importances_)\n",
    "\n",
    "    return (accuracy_score(y_test,y_test_pred), y_test_pred,best_params, clf.feature_importances_)\n",
    "def l1_model(X_other,y_other, X_test, y_test,random_state, verbose=1):\n",
    "    # make into row vectors to avoid an obnoxious sklearn/xgb warning\n",
    "    n_splits = 5\n",
    "    this_cv = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=random_state)\n",
    "    y_other = np.reshape(np.array(y_other), (1, -1)).ravel()\n",
    "    y_test = np.reshape(np.array(y_test), (1, -1)).ravel()\n",
    "\n",
    "    clf = LogisticRegression(solver='saga', penalty='l1',n_jobs=-1,random_state=random_state)\n",
    "    \n",
    "    # find the best parameter set\n",
    "    param_grid = {\n",
    "    'C': 1/np.logspace(-2,2,num=5)              # Regularization parameter\n",
    "}\n",
    "\n",
    "    pg = ParameterGrid(param_grid)\n",
    "\n",
    "    scores = np.zeros((len(pg),n_splits))\n",
    "\n",
    "    for i in range(len(pg)):\n",
    "        if verbose >= 5:\n",
    "                print(\"Param set \" + str(i + 1) + \" / \" + str(len(pg)))\n",
    "        params = pg[i]\n",
    "        for j, (train_index, val_index) in enumerate(this_cv.split(X_other,y_other)):\n",
    "            X_train = X_other.iloc[train_index,:]\n",
    "            y_train = y_other[train_index]\n",
    "            X_val = X_other.iloc[val_index,:]\n",
    "            y_val = y_other[val_index]\n",
    "            clf.set_params(**params)\n",
    "            clf.fit(X_train, y_train)# with early stopping\n",
    "            y_val_pred = clf.predict(X_val)\n",
    "            scores[i,j] = accuracy_score(y_val,y_val_pred)\n",
    "    scores = np.mean(scores, axis=1)\n",
    "\n",
    "    best_params = np.array(pg)[scores == np.max(scores)]\n",
    "    if verbose >= 4:\n",
    "        print('Test set max score and best parameters are:')\n",
    "        print(np.max(scores))\n",
    "        print(best_params)\n",
    "\n",
    "    # test the model on the test set with best parameter set\n",
    "    clf.set_params(**best_params[0])\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print ('The accuracy is:',accuracy_score(y_test,y_test_pred))\n",
    "    if verbose >= 2:\n",
    "        print ('The predictions are:')\n",
    "        print (y_test_pred)\n",
    "    if verbose >= 3:\n",
    "        print(\"Feature importances:\")\n",
    "        print(clf.feature_importances_)\n",
    "\n",
    "    return (accuracy_score(y_test,y_test_pred), y_test_pred,best_params)\n",
    "\n",
    "def l2_model(X_other,y_other, X_test, y_test, random_state,verbose=1):\n",
    "    # make into row vectors to avoid an obnoxious sklearn/xgb warning\n",
    "    n_splits = 5\n",
    "    this_cv = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=random_state)\n",
    "    y_other = np.reshape(np.array(y_other), (1, -1)).ravel()\n",
    "    y_test = np.reshape(np.array(y_test), (1, -1)).ravel()\n",
    "\n",
    "    clf = LogisticRegression(solver='saga', penalty='l2',n_jobs=-1,random_state=random_state)\n",
    "    \n",
    "    # find the best parameter set\n",
    "    param_grid = {\n",
    "    'C': 1/np.logspace(-2,2,num=5)              # Regularization parameter\n",
    "}\n",
    "\n",
    "    pg = ParameterGrid(param_grid)\n",
    "\n",
    "    scores = np.zeros((len(pg),n_splits))\n",
    "\n",
    "    for i in range(len(pg)):\n",
    "        if verbose >= 5:\n",
    "                print(\"Param set \" + str(i + 1) + \" / \" + str(len(pg)))\n",
    "        params = pg[i]\n",
    "        for j, (train_index, val_index) in enumerate(this_cv.split(X_other, y_other)):\n",
    "            X_train = X_other.iloc[train_index,:]\n",
    "            y_train = y_other[train_index]\n",
    "            X_val = X_other.iloc[val_index,:]\n",
    "            y_val = y_other[val_index]\n",
    "            clf.set_params(**params)\n",
    "            clf.fit(X_train, y_train)# with early stopping\n",
    "            y_val_pred = clf.predict(X_val)\n",
    "            scores[i,j] = accuracy_score(y_val,y_val_pred)\n",
    "    scores = np.mean(scores, axis=1)\n",
    "\n",
    "    best_params = np.array(pg)[scores == np.max(scores)]\n",
    "    if verbose >= 4:\n",
    "        print('Test set max score and best parameters are:')\n",
    "        print(np.max(scores))\n",
    "        print(best_params)\n",
    "\n",
    "    # test the model on the test set with best parameter set\n",
    "    clf.set_params(**best_params[0])\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print ('The accuracy is:',accuracy_score(y_test,y_test_pred))\n",
    "    if verbose >= 2:\n",
    "        print ('The predictions are:')\n",
    "        print (y_test_pred)\n",
    "    if verbose >= 3:\n",
    "        print(\"Feature importances:\")\n",
    "        print(clf.feature_importances_)\n",
    "\n",
    "    return (accuracy_score(y_test,y_test_pred), y_test_pred,best_params)\n",
    "\n",
    "def ela_model(X_other,y_other, X_test, y_test,random_state, verbose=1):\n",
    "    # make into row vectors to avoid an obnoxious sklearn/xgb warning\n",
    "    n_splits = 5\n",
    "    this_cv = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=random_state)\n",
    "    y_other = np.reshape(np.array(y_other), (1, -1)).ravel()\n",
    "    y_test = np.reshape(np.array(y_test), (1, -1)).ravel()\n",
    "\n",
    "    clf = LogisticRegression(solver='saga', penalty='elasticnet',n_jobs=-1,random_state=random_state)\n",
    "    \n",
    "    # find the best parameter set\n",
    "    param_grid = {\n",
    "    'C': 1/np.logspace(-2,2,num=5),              # Regularization parameter\n",
    "    'l1_ratio': np.linspace(0,1,5)\n",
    "}\n",
    "\n",
    "    pg = ParameterGrid(param_grid)\n",
    "\n",
    "    scores = np.zeros((len(pg),n_splits))\n",
    "\n",
    "    for i in range(len(pg)):\n",
    "        if verbose >= 5:\n",
    "                print(\"Param set \" + str(i + 1) + \" / \" + str(len(pg)))\n",
    "        params = pg[i]\n",
    "        for j, (train_index, val_index) in enumerate(this_cv.split(X_other,y_other)):\n",
    "            X_train = X_other.iloc[train_index,:]\n",
    "            y_train = y_other[train_index]\n",
    "            X_val = X_other.iloc[val_index,:]\n",
    "            y_val = y_other[val_index]\n",
    "            clf.set_params(**params)\n",
    "            clf.fit(X_train, y_train)# with early stopping\n",
    "            y_val_pred = clf.predict(X_val)\n",
    "            scores[i,j] = accuracy_score(y_val,y_val_pred)\n",
    "    scores = np.mean(scores, axis=1)\n",
    "\n",
    "    best_params = np.array(pg)[scores == np.max(scores)]\n",
    "    if verbose >= 4:\n",
    "        print('Test set max score and best parameters are:')\n",
    "        print(np.max(scores))\n",
    "        print(best_params)\n",
    "\n",
    "    # test the model on the test set with best parameter set\n",
    "    clf.set_params(**best_params[0])\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print ('The accuracy is:',accuracy_score(y_test,y_test_pred))\n",
    "    if verbose >= 2:\n",
    "        print ('The predictions are:')\n",
    "        print (y_test_pred)\n",
    "    if verbose >= 3:\n",
    "        print(\"Feature importances:\")\n",
    "        print(clf.feature_importances_)\n",
    "\n",
    "    return (accuracy_score(y_test,y_test_pred), y_test_pred,best_params)\n",
    "\n",
    "# random forest, svm, decision tree, l1, l2,\n",
    "# neural network\n",
    "\n",
    "def rf_model(X_other,y_other, X_test, y_test, random_state,verbose=1):\n",
    "    # make into row vectors to avoid an obnoxious sklearn/xgb warning\n",
    "    n_splits = 5\n",
    "    this_cv = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=random_state) # 分层？\n",
    "    y_other = np.reshape(np.array(y_other), (1, -1)).ravel()\n",
    "    y_test = np.reshape(np.array(y_test), (1, -1)).ravel()\n",
    "\n",
    "    clf = RandomForestClassifier(n_jobs=-1,random_state=random_state)\n",
    "    \n",
    "    # find the best parameter set\n",
    "    param_grid = {'n_estimators': [100], 'max_features': ['log2', 'sqrt'],'max_depth': [2,4,8,16,32]}\n",
    "\n",
    "    pg = ParameterGrid(param_grid)\n",
    "\n",
    "    scores = np.zeros((len(pg),n_splits))\n",
    "\n",
    "    for i in range(len(pg)):\n",
    "        if verbose >= 5:\n",
    "                print(\"Param set \" + str(i + 1) + \" / \" + str(len(pg)))\n",
    "        params = pg[i]\n",
    "        for j, (train_index, val_index) in enumerate(this_cv.split(X_other,y_other)):\n",
    "            X_train = X_other.iloc[train_index,:]\n",
    "            y_train = y_other[train_index]\n",
    "            X_val = X_other.iloc[val_index,:]\n",
    "            y_val = y_other[val_index]\n",
    "            clf.set_params(**params)\n",
    "            clf.fit(X_train, y_train)# with early stopping\n",
    "            y_val_pred = clf.predict(X_val)\n",
    "            scores[i,j] = accuracy_score(y_val,y_val_pred)\n",
    "    scores = np.mean(scores, axis=1)\n",
    "\n",
    "    best_params = np.array(pg)[scores == np.max(scores)]\n",
    "    if verbose >= 4:\n",
    "        print('Test set max score and best parameters are:')\n",
    "        print(np.max(scores))\n",
    "        print(best_params)\n",
    "\n",
    "    # test the model on the test set with best parameter set\n",
    "    clf.set_params(**best_params[0])\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print ('The accuracy is:',accuracy_score(y_test,y_test_pred))\n",
    "    if verbose >= 2:\n",
    "        print ('The predictions are:')\n",
    "        print (y_test_pred)\n",
    "    if verbose >= 3:\n",
    "        print(\"Feature importances:\")\n",
    "        print(clf.feature_importances_)\n",
    "\n",
    "    return (accuracy_score(y_test,y_test_pred), y_test_pred,best_params, clf.feature_importances_)\n",
    "\n",
    "def knn_model(X_other,y_other, X_test, y_test,random_state, verbose=1):\n",
    "    # make into row vectors to avoid an obnoxious sklearn/xgb warning\n",
    "    n_splits = 5\n",
    "    this_cv = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=random_state)\n",
    "    y_other = np.reshape(np.array(y_other), (1, -1)).ravel()\n",
    "    y_test = np.reshape(np.array(y_test), (1, -1)).ravel()\n",
    "\n",
    "    clf = KNeighborsClassifier(n_jobs=-1)\n",
    "    \n",
    "    # find the best parameter set\n",
    "    param_grid = {'n_neighbors':[2,4,8,16,32,64]}\n",
    "\n",
    "    pg = ParameterGrid(param_grid)\n",
    "\n",
    "    scores = np.zeros((len(pg),n_splits))\n",
    "\n",
    "    for i in range(len(pg)):\n",
    "        if verbose >= 5:\n",
    "                print(\"Param set \" + str(i + 1) + \" / \" + str(len(pg)))\n",
    "        params = pg[i]\n",
    "        for j, (train_index, val_index) in enumerate(this_cv.split(X_other,y_other)):\n",
    "            X_train = X_other.iloc[train_index,:]\n",
    "            y_train = y_other[train_index]\n",
    "            X_val = X_other.iloc[val_index,:]\n",
    "            y_val = y_other[val_index]\n",
    "            clf.set_params(**params)\n",
    "            clf.fit(X_train, y_train)# with early stopping\n",
    "            y_val_pred = clf.predict(X_val)\n",
    "            scores[i,j] = accuracy_score(y_val,y_val_pred)\n",
    "    scores = np.mean(scores, axis=1)\n",
    "\n",
    "    best_params = np.array(pg)[scores == np.max(scores)]\n",
    "    if verbose >= 4:\n",
    "        print('Test set max score and best parameters are:')\n",
    "        print(np.max(scores))\n",
    "        print(best_params)\n",
    "\n",
    "    # test the model on the test set with best parameter set\n",
    "    clf.set_params(**best_params[0])\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print ('The accuracy is:',accuracy_score(y_test,y_test_pred))\n",
    "    if verbose >= 2:\n",
    "        print ('The predictions are:')\n",
    "        print (y_test_pred)\n",
    "    if verbose >= 3:\n",
    "        print(\"Feature importances:\")\n",
    "        print(clf.feature_importances_)\n",
    "\n",
    "    return (accuracy_score(y_test,y_test_pred), y_test_pred,best_params)\n",
    "\n",
    "def svc_model(X_other,y_other, X_test, y_test, random_state,verbose=1):\n",
    "    # make into row vectors to avoid an obnoxious sklearn/xgb warning\n",
    "    n_splits = 5\n",
    "    this_cv = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=random_state)\n",
    "    y_other = np.reshape(np.array(y_other), (1, -1)).ravel()\n",
    "    y_test = np.reshape(np.array(y_test), (1, -1)).ravel()\n",
    "\n",
    "    clf = SVC(random_state=random_state)\n",
    "    \n",
    "    # find the best parameter set\n",
    "    param_grid = {\n",
    "    'C': np.logspace(-2,2,num=5),              # Regularization parameter\n",
    "    'gamma':np.logspace(-2,2,num=5),           # Gamma values for rbf/poly/sigmoid kernels\n",
    "}\n",
    "\n",
    "    pg = ParameterGrid(param_grid)\n",
    "\n",
    "    scores = np.zeros((len(pg),n_splits))\n",
    "\n",
    "    for i in range(len(pg)):\n",
    "        if verbose >= 5:\n",
    "                print(\"Param set \" + str(i + 1) + \" / \" + str(len(pg)))\n",
    "        params = pg[i]\n",
    "        for j, (train_index, val_index) in enumerate(this_cv.split(X_other,y_other)):\n",
    "            X_train = X_other.iloc[train_index,:]\n",
    "            y_train = y_other[train_index]\n",
    "            X_val = X_other.iloc[val_index,:]\n",
    "            y_val = y_other[val_index]\n",
    "            clf.set_params(**params)\n",
    "            clf.fit(X_train, y_train)# with early stopping\n",
    "            y_val_pred = clf.predict(X_val)\n",
    "            scores[i,j] = accuracy_score(y_val,y_val_pred)\n",
    "    scores = np.mean(scores, axis=1)\n",
    "\n",
    "    best_params = np.array(pg)[scores == np.max(scores)]\n",
    "    if verbose >= 4:\n",
    "        print('Test set max score and best parameters are:')\n",
    "        print(np.max(scores))\n",
    "        print(best_params)\n",
    "\n",
    "    # test the model on the test set with best parameter set\n",
    "    clf.set_params(**best_params[0])\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print ('The accuracy is:',accuracy_score(y_test,y_test_pred))\n",
    "    if verbose >= 2:\n",
    "        print ('The predictions are:')\n",
    "        print (y_test_pred)\n",
    "    if verbose >= 3:\n",
    "        print(\"Feature importances:\")\n",
    "        print(clf.feature_importances_)\n",
    "\n",
    "    return (accuracy_score(y_test,y_test_pred), y_test_pred,best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for random_state in range(0,3):\n",
    "        X_other,  X_test, y_other, y_test = train_test_split(X,y,train_size=0.2,shuffle=True,stratify=y)\n",
    "        le = LabelEncoder() \n",
    "        y_other = le.fit_transform(y_other)\n",
    "        y_test = le.transform(y_test)\n",
    "        total_acc, y_test_pred, best_params, importance =  xgb_model(pd.DataFrame(X_other),pd.DataFrame(y_other), pd.DataFrame(X_test), pd.DataFrame(y_test),random_state, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6654939487756826)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-y_test.sum()/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Citizens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
