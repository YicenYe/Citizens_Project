{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, ParameterGrid\n",
    "from sklearn.metrics import (\n",
    "    recall_score, precision_score, f1_score,\n",
    "    accuracy_score, roc_auc_score, classification_report\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df = pd.read_csv('../data/ywan1077/citizens_data/BrownDSI_masked_capstone_data.csv_20250401031515')\n",
    "    # df = pd.read_csv('/users/ywan1077/data/yye45/citizens_data/BrownDSI_masked_capstone_data.csv_20250401031515')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>masked_dep_acct_num</th>\n",
       "      <th>masked_bank_num</th>\n",
       "      <th>masked_account_type</th>\n",
       "      <th>masked_id</th>\n",
       "      <th>masked_product_code</th>\n",
       "      <th>bucket_days_since_open</th>\n",
       "      <th>number_of_owners</th>\n",
       "      <th>total_deposit_amount</th>\n",
       "      <th>item_amt</th>\n",
       "      <th>deposit_dt</th>\n",
       "      <th>...</th>\n",
       "      <th>drawee_sum</th>\n",
       "      <th>drawee_cnt</th>\n",
       "      <th>drawee_avg</th>\n",
       "      <th>drawee_max</th>\n",
       "      <th>drawee_min</th>\n",
       "      <th>RDI_DT</th>\n",
       "      <th>RETURN_REASON</th>\n",
       "      <th>return_target</th>\n",
       "      <th>over_draft_amount</th>\n",
       "      <th>month_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5000+</td>\n",
       "      <td>1</td>\n",
       "      <td>517.81</td>\n",
       "      <td>517.81</td>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1882.21</td>\n",
       "      <td>4.0</td>\n",
       "      <td>470.5525</td>\n",
       "      <td>535.92</td>\n",
       "      <td>400.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0-1000</td>\n",
       "      <td>1</td>\n",
       "      <td>150.00</td>\n",
       "      <td>150.00</td>\n",
       "      <td>2024-04-25</td>\n",
       "      <td>...</td>\n",
       "      <td>150.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150.0000</td>\n",
       "      <td>150.00</td>\n",
       "      <td>150.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5000+</td>\n",
       "      <td>0</td>\n",
       "      <td>2603.24</td>\n",
       "      <td>2451.00</td>\n",
       "      <td>2024-03-18</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5000+</td>\n",
       "      <td>0</td>\n",
       "      <td>770.00</td>\n",
       "      <td>770.00</td>\n",
       "      <td>2024-11-29</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2000-5000</td>\n",
       "      <td>2</td>\n",
       "      <td>10776.63</td>\n",
       "      <td>145.00</td>\n",
       "      <td>2024-05-29</td>\n",
       "      <td>...</td>\n",
       "      <td>7860.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7860.0000</td>\n",
       "      <td>7860.00</td>\n",
       "      <td>7860.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   masked_dep_acct_num  masked_bank_num  masked_account_type  masked_id  \\\n",
       "0                    0                0                    0          0   \n",
       "1                    2                0                    0          2   \n",
       "2                    3                0                    0          3   \n",
       "3                    4                0                    0          4   \n",
       "4                    7                0                    0          7   \n",
       "\n",
       "   masked_product_code bucket_days_since_open  number_of_owners  \\\n",
       "0                    0                  5000+                 1   \n",
       "1                    1                 0-1000                 1   \n",
       "2                    0                  5000+                 0   \n",
       "3                    1                  5000+                 0   \n",
       "4                    1              2000-5000                 2   \n",
       "\n",
       "   total_deposit_amount  item_amt  deposit_dt  ... drawee_sum  drawee_cnt  \\\n",
       "0                517.81    517.81  2024-08-01  ...    1882.21         4.0   \n",
       "1                150.00    150.00  2024-04-25  ...     150.00         1.0   \n",
       "2               2603.24   2451.00  2024-03-18  ...        NaN         NaN   \n",
       "3                770.00    770.00  2024-11-29  ...        NaN         NaN   \n",
       "4              10776.63    145.00  2024-05-29  ...    7860.00         1.0   \n",
       "\n",
       "  drawee_avg drawee_max drawee_min RDI_DT  RETURN_REASON  return_target  \\\n",
       "0   470.5525     535.92     400.77    NaN            NaN              0   \n",
       "1   150.0000     150.00     150.00    NaN            NaN              0   \n",
       "2        NaN        NaN        NaN    NaN            NaN              0   \n",
       "3        NaN        NaN        NaN    NaN            NaN              0   \n",
       "4  7860.0000    7860.00    7860.00    NaN            NaN              0   \n",
       "\n",
       "   over_draft_amount  month_num  \n",
       "0                  0          8  \n",
       "1                  0          4  \n",
       "2                  0          3  \n",
       "3                  0         11  \n",
       "4                  0          5  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_pipeline(df):\n",
    "    \"\"\"\n",
    "    XGBoost Feature Engineering Pipeline\n",
    "\n",
    "    This pipeline performs minimal cleaning and targeted feature engineering\n",
    "    to prepare the dataset for XGBoost modeling. Specifically, it:\n",
    "\n",
    "    - Drops irrelevant or identifier columns that are not useful for prediction\n",
    "    - Extracts temporal features (quarter, week) from the deposit date\n",
    "    - Creates ratio-based and volatility-based transaction features\n",
    "    - Creates frequency-based features from historical transaction dates\n",
    "    - Creates behavioral flags based on deposit amount anomalies\n",
    "    - Processes bucketed and binary categorical features into numeric form\n",
    "    - Preserves missing values (np.nan) without imputation\n",
    "    - Does not perform feature scaling or one-hot encoding\n",
    "    - Leaves outlier values as-is without clipping\n",
    "\n",
    "    The resulting dataset is fully numeric, with missing values retained, and ready for XGBoost models.\n",
    "    \"\"\"\n",
    "\n",
    "    cols_to_drop = [\n",
    "        'masked_dep_acct_num',\n",
    "        'masked_id',\n",
    "        'channel',\n",
    "        'RDI_DT',\n",
    "        'RETURN_REASON',\n",
    "        'over_draft_amount',\n",
    "        'return_target'  \n",
    "    ]\n",
    "\n",
    "    df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "    # --------------- process deposit_dt columns --------------- \n",
    "    df['deposit_dt'] = pd.to_datetime(df['deposit_dt'], errors='coerce')\n",
    "    df['fe_deposit_quarter'] = df['deposit_dt'].dt.quarter\n",
    "    df['fe_deposit_week'] = df['deposit_dt'].dt.isocalendar().week\n",
    "    df = df.drop(columns=['deposit_dt'])\n",
    "\n",
    "\n",
    "    # --------------- process item_amt, total_deposit_amount, relationship_balance columns --------------- \n",
    "    ## Ratio of current deposit item amount to total deposit amount, large single deposits are potentially unusual 如果一笔交易占今天总交易很大一部分会不会是诈骗\n",
    "    epsilon = 1e-6\n",
    "    denom_balance = df['relationship_balance']\n",
    "    denom_balance = np.where(denom_balance <= 0, epsilon, denom_balance)\n",
    "\n",
    "    df['fe_amt_ratio_to_total'] = df['item_amt'] / (df['total_deposit_amount'] + 1)\n",
    "    ## Ratio of current deposit item amount to prior day's total balance, Some frauds are small balance with large deposits all of a sudden. 如果一笔交易占用户总余额很大一部分会不会是诈骗\n",
    "    df['fe_amt_ratio_to_balance'] = (df['item_amt'] / denom_balance).clip(upper=1e6)\n",
    "    ## Ratio of total deposit amount to prior day's total balance, Some frauds are small balance with large deposits all of a sudden. 如果今天的总交易占用户总余额很大一部分会不会是诈骗\n",
    "    df['fe_total_amt_ratio_to_balance'] = (df['total_deposit_amount'] / denom_balance).clip(upper=1e6)\n",
    "    ## Difference between current deposit amount and previous transaction amount, abnormal fluctuations 如果这笔交易比上一笔变化很多会不会是诈骗\n",
    "    df['fe_amt_change_from_prev'] = df['item_amt'] - df['prevtran1']\n",
    "    ## Flag indicating whether the current deposit item is 1.5x larger than the past 30-day maximum deposit, Is it significantly higher than the largest deposit in the last 30 days 如果一笔交易是这一个月的最大交易会不会是诈骗\n",
    "    df['fe_high_amt_flag'] = (df['item_amt'] > (df['max_deposit_amount30d'] * 1.5)).astype(int)\n",
    "    ## Ratio of maximum single deposit amount in past 30 days to the total deposit amount \n",
    "    df['fe_max30d_ratio_total'] = df['max_deposit_amount30d'] / (df['total_deposit_amount'] + 1)\n",
    "\n",
    "    # --------------- process rdis columns --------------- \n",
    "    ## fill NA as 0 to rdis\n",
    "    df['rdis'] = df['rdis'].fillna(0)\n",
    "    df['fe_rdis_flag'] = (df['rdis'] > 0).astype(int)\n",
    "\n",
    "    # --------------- process prevtrandate columns --------------- \n",
    "    ## process prevtrandate to capture short-term transaction frequency.\n",
    "    prevtrandate_cols = [f'prevtrandate{i}' for i in range(1, 11)]\n",
    "    prevtran_cols = [f'prevtran{i}' for i in range(1, 11)]\n",
    "    # Create transaction count features\n",
    "    ## Number of transactions within 1 day\n",
    "    df['fe_num_trans_in_1d'] = (df[prevtrandate_cols] <= 1).sum(axis=1)\n",
    "    ## Number of transactions within 5 days\n",
    "    df['fe_num_trans_in_5d'] = (df[prevtrandate_cols] <= 5).sum(axis=1)\n",
    "    ## Number of transactions within 10 days\n",
    "    df['fe_num_trans_in_10d'] = (df[prevtrandate_cols] <= 10).sum(axis=1)\n",
    "    ## Average days between previous transactions\n",
    "    df['fe_avg_prevtrandate'] = df[prevtrandate_cols].mean(axis=1)\n",
    "    ## Standard deviation of days between previous transactions\n",
    "    df['fe_std_prevtrandate'] = df[prevtrandate_cols].std(axis=1)\n",
    "    ## Rolling mean of the most recent 3 and 5 previous transaction days\n",
    "    df['fe_rolling_mean_prevtrandate_3'] = df[prevtrandate_cols[:3]].mean(axis=1)\n",
    "    df['fe_rolling_mean_prevtrandate_5'] = df[prevtrandate_cols[:5]].mean(axis=1)\n",
    "    ## Rolling standard deviation of the most recent 3 and 5 previous transaction days\n",
    "    df['fe_rolling_std_prevtrandate_3'] = df[prevtrandate_cols[:3]].std(axis=1)\n",
    "    df['fe_rolling_std_prevtrandate_5'] = df[prevtrandate_cols[:5]].std(axis=1)\n",
    "\n",
    "\n",
    "    # ------------ process prevtran columns ------------\n",
    "    # volatility of item_amt compared to historical transaction amounts\n",
    "    ## Mean of previous 10 deposits  ps If there is all NaN, the result of std is NaN!\n",
    "    df['fe_avg_prevtran_amt'] = df[prevtran_cols].mean(axis=1)\n",
    "    ## Standard deviation of previous 10 deposits ps If there is only 1 valid number or all NaN, the result of std is NaN!\n",
    "    df['fe_std_prevtran_amt'] = df[prevtran_cols].std(axis=1)\n",
    "    ## Maximum amount among previous 10 deposits ps If there is all NaN, the result of std is NaN!\n",
    "    df['fe_max_prevtran_amt'] = df[prevtran_cols].max(axis=1)\n",
    "    ## Current deposit amount compared to historical average \n",
    "    df['fe_amt_current_vs_avg'] = df['item_amt'] / (df['fe_avg_prevtran_amt'] + 1)\n",
    "    ## Current deposit amount compared to historical maximum \n",
    "    df['fe_amt_current_vs_max'] = df['item_amt'] / (df['fe_max_prevtran_amt'] + 1)\n",
    "    # Rolling part to capture short-term behavioral trends from the latest 3 or 5 transactions.\n",
    "    # Rolling mean of the most recent 3 and 5 previous transactions ps If there is all NaN, the result of std is NaN!\n",
    "    df['fe_rolling_mean_prevtran_3'] = df[prevtran_cols[:3]].mean(axis=1)\n",
    "    df['fe_rolling_mean_prevtran_5'] = df[prevtran_cols[:5]].mean(axis=1)\n",
    "    # Rolling standard deviation of the most recent 3 and 5 previous transactions ps if If there is only 1 valid number or all NaN, the result is NaN!\n",
    "    df['fe_rolling_std_prevtran_3'] = df[prevtran_cols[:3]].std(axis=1)\n",
    "    df['fe_rolling_std_prevtran_5'] = df[prevtran_cols[:5]].std(axis=1)\n",
    "    ## Create a feature indicating whether the current deposit is the first known deposit\n",
    "    df['fe_is_first_deposit'] = df[prevtran_cols].isna().all(axis=1).astype(int)\n",
    "\n",
    "\n",
    "    # --------------- Label Encoding ---------------\n",
    "    ##  --------------- process mapping_bucket_days columns to approximate days(med) --------------- \n",
    "    mapping_bucket_days = {\n",
    "        '0-1000': 500,\n",
    "        '1000-2000': 1500,\n",
    "        '2000-5000': 3500,\n",
    "        '5000+': 6000\n",
    "    }\n",
    "    df['fe_days_since_open'] = df['bucket_days_since_open'].map(mapping_bucket_days)\n",
    "    df = df.drop(columns=['bucket_days_since_open'])\n",
    "    ## --------------- process oao_flg columns ---------------\n",
    "    mapping_oao_flg = {\n",
    "        'N': 0,\n",
    "        'Y': 1\n",
    "    }\n",
    "    df['fe_oao_flg'] = df['oao_flg'].map(mapping_oao_flg)\n",
    "    df = df.drop(columns=['oao_flg'])\n",
    "    ## --------------- process onus_ind columns ---------------\n",
    "    mapping_onus_ind = {\n",
    "        'F': 0,\n",
    "        'T': 1\n",
    "    }\n",
    "    df['fe_onus_ind'] = df['onus_ind'].map(mapping_onus_ind)\n",
    "    df = df.drop(columns=['onus_ind'])\n",
    "    ## --------------- process treasury_check_ind columns ---------------\n",
    "    mapping_treasury_check_ind = {\n",
    "        'N': 0,\n",
    "        'Y': 1\n",
    "    }\n",
    "    df['fe_treasury_check_ind'] = df['treasury_check_ind'].map(mapping_treasury_check_ind)\n",
    "    df = df.drop(columns=['treasury_check_ind'])\n",
    "    ## --------------- process heloc_ind columns ---------------\n",
    "    mapping_heloc_ind = {\n",
    "        'N': 0,\n",
    "        'Y': 1\n",
    "    }\n",
    "    df['fe_heloc_ind'] = df['heloc_ind'].map(mapping_heloc_ind)\n",
    "    df = df.drop(columns=['heloc_ind'])\n",
    "\n",
    "    \n",
    "    df_xgboost = df.copy()\n",
    "\n",
    "    return df_xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = xgboost_pipeline(df)\n",
    "\n",
    "X = df_processed\n",
    "y = df['return_target']  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14212, 69)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_pos_weight 应设为： 1.99\n"
     ]
    }
   ],
   "source": [
    "pos = sum(y_train == 1)\n",
    "neg = sum(y_train == 0)\n",
    "print(\"scale_pos_weight：\", round(neg / pos, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_eval_xgb(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    y_pred_bin = (y_pred > 0.5).astype(int)\n",
    "    recall = recall_score(y_true, y_pred_bin)\n",
    "    return 'recall', recall\n",
    "\n",
    "def tune_xgb_native(X_train, y_train, X_test, y_test, thresholds=[0.5, 0.6, 0.7], verbose=True):\n",
    "    # Split validation set\n",
    "    X_fit, X_val, y_fit, y_val = train_test_split(\n",
    "        X_train, y_train,\n",
    "        test_size=0.2,\n",
    "        stratify=y_train,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # ================== Stage 1: Coarse tuning ==================\n",
    "    param_grid_stage1 = {\n",
    "    \"learning_rate\":      [0.01, 0.015],         # 0.01\n",
    "    \"max_depth\":          [3, 5, 7],                   \n",
    "    \"min_child_weight\":   [1],                       # 1\n",
    "    \"subsample\":          [0.55],             # 0.55\n",
    "    \"colsample_bytree\":   [0.55],             # 0.55\n",
    "    \"gamma\":              [0.1, 0.2, 0.5],                      \n",
    "    \"lambda\":             [1, 3, 5],               \n",
    "    \"alpha\":              [1, 15, 30],        \n",
    "    \"scale_pos_weight\":   [2.5, 3.5]               # 3.5\n",
    "    }\n",
    "\n",
    "\n",
    "    pg1 = list(ParameterGrid(param_grid_stage1))\n",
    "    scores1 = []\n",
    "\n",
    "    for params in pg1:\n",
    "        fold_scores = []\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        for train_idx, val_idx in cv.split(X_fit, y_fit):\n",
    "            X_tr, X_va = X_fit.iloc[train_idx], X_fit.iloc[val_idx]\n",
    "            y_tr, y_va = y_fit.iloc[train_idx], y_fit.iloc[val_idx]\n",
    "            dtr = xgb.DMatrix(X_tr, label=y_tr)\n",
    "            dva = xgb.DMatrix(X_va, label=y_va)\n",
    "\n",
    "            p = dict(params)\n",
    "            p['objective'] = 'binary:logistic'\n",
    "            # p['eval_metric'] = 'logloss'\n",
    "\n",
    "            \n",
    "            booster = xgb.train(\n",
    "                params=p,\n",
    "                dtrain=dtr,\n",
    "                evals=[(dva, 'val')],\n",
    "                num_boost_round=800,\n",
    "                early_stopping_rounds=50,\n",
    "                custom_metric=recall_eval_xgb,\n",
    "                maximize=True,\n",
    "                verbose_eval=False\n",
    "            )\n",
    "\n",
    "            preds = booster.predict(dva)\n",
    "            fold_scores.append(recall_score(y_va, preds > 0.5))\n",
    "        scores1.append(np.mean(fold_scores))\n",
    "\n",
    "    best_stage1_params = pg1[np.argmax(scores1)]\n",
    "    if verbose:\n",
    "        print(\"\\n[Stage 1] Best parameters:\", best_stage1_params)\n",
    "\n",
    "    # ================== Stage 2: Fine tuning ==================\n",
    "    base = best_stage1_params\n",
    "    param_grid_stage2 = {\n",
    "        \"learning_rate\":      [base[\"learning_rate\"],\n",
    "                            round(base[\"learning_rate\"]*0.8, 3),\n",
    "                            round(base[\"learning_rate\"]*1.2, 3)],\n",
    "        \"max_depth\":          [max(2, base[\"max_depth\"]-1),\n",
    "                            base[\"max_depth\"],\n",
    "                            min(6, base[\"max_depth\"]+1)],\n",
    "        \"min_child_weight\":   [max(1, base[\"min_child_weight\"]-2),\n",
    "                            base[\"min_child_weight\"],\n",
    "                            base[\"min_child_weight\"]+2],\n",
    "        \"gamma\":              [base[\"gamma\"]],\n",
    "        \"subsample\":          [max(0.5, round(base[\"subsample\"]-0.05, 2)),\n",
    "                            base[\"subsample\"],\n",
    "                            min(0.9, round(base[\"subsample\"]+0.05, 2))],\n",
    "        \"colsample_bytree\":   [max(0.5, round(base[\"colsample_bytree\"]-0.05, 2)),\n",
    "                            base[\"colsample_bytree\"],\n",
    "                            min(0.9, round(base[\"colsample_bytree\"]+0.05, 2))],\n",
    "        \"lambda\":             [base[\"lambda\"],\n",
    "                            round(base[\"lambda\"]*0.7, 3),\n",
    "                            round(base[\"lambda\"]*1.4, 3)],\n",
    "        \"alpha\":              [base[\"alpha\"],\n",
    "                            round(base[\"alpha\"]*0.7, 3),\n",
    "                            round(base[\"alpha\"]*1.4, 3)],\n",
    "        \"scale_pos_weight\":   [base[\"scale_pos_weight\"]]\n",
    "    }\n",
    "\n",
    "    pg2 = list(ParameterGrid(param_grid_stage2))\n",
    "    scores2 = []\n",
    "\n",
    "    for params in pg2:\n",
    "        fold_scores = []\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        for train_idx, val_idx in cv.split(X_fit, y_fit):\n",
    "            X_tr, X_va = X_fit.iloc[train_idx], X_fit.iloc[val_idx]\n",
    "            y_tr, y_va = y_fit.iloc[train_idx], y_fit.iloc[val_idx]\n",
    "            dtr = xgb.DMatrix(X_tr, label=y_tr)\n",
    "            dva = xgb.DMatrix(X_va, label=y_va)\n",
    "\n",
    "            p = dict(params)\n",
    "            p['objective'] = 'binary:logistic'\n",
    "            booster = xgb.train(\n",
    "                params=p,\n",
    "                dtrain=dtr,\n",
    "                num_boost_round=1000,\n",
    "                evals=[(dva, 'val')],\n",
    "                custom_metric=recall_eval_xgb,\n",
    "                early_stopping_rounds=50,\n",
    "                maximize=True,\n",
    "                verbose_eval=False\n",
    "            )\n",
    "\n",
    "            preds = booster.predict(dva)\n",
    "            fold_scores.append(recall_score(y_va, preds > 0.5))\n",
    "        scores2.append(np.mean(fold_scores))\n",
    "\n",
    "    best_params = pg2[np.argmax(scores2)]\n",
    "    if verbose:\n",
    "        print(\"\\n[Stage 2] Best tuned parameters:\", best_params)\n",
    "\n",
    "    # ================== Final Model Training ==================\n",
    "    evals_result = {}\n",
    "    dfinal_train = xgb.DMatrix(X_fit, label=y_fit)\n",
    "    dfinal_val = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "    final_model = xgb.train(\n",
    "    params={**best_params, \"objective\": \"binary:logistic\"},\n",
    "    dtrain=dfinal_train,\n",
    "    evals=[(dfinal_val, \"val\")],\n",
    "    num_boost_round=2000,\n",
    "    custom_metric=recall_eval_xgb,\n",
    "    maximize=True, \n",
    "    early_stopping_rounds=50,\n",
    "    evals_result=evals_result,\n",
    "    verbose_eval=verbose\n",
    "    )\n",
    "\n",
    "    # ================== Test Set Evaluation ==================\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    y_prob = final_model.predict(dtest)\n",
    "    print(\"\\n[Test Set Evaluation]\")\n",
    "    print(f\"AUC: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_prob > threshold).astype(int)\n",
    "        p = precision_score(y_test, y_pred)\n",
    "        r = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        print(f\"\\n---- Threshold = {threshold:.2f} ----\")\n",
    "        print(f\"Precision: {p:.4f}, Recall: {r:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    return final_model, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Stage 1] Best parameters: {'alpha': 15, 'colsample_bytree': 0.55, 'gamma': 0.1, 'lambda': 1, 'learning_rate': 0.01, 'max_depth': 3, 'min_child_weight': 1, 'scale_pos_weight': 3.5, 'subsample': 0.55}\n",
      "\n",
      "[Stage 2] Best tuned parameters: {'alpha': 15, 'colsample_bytree': 0.5, 'gamma': 0.1, 'lambda': 1, 'learning_rate': 0.01, 'max_depth': 2, 'min_child_weight': 1, 'scale_pos_weight': 3.5, 'subsample': 0.5}\n",
      "[0]\tval-logloss:0.82109\tval-recall:1.00000\n",
      "[1]\tval-logloss:0.82050\tval-recall:1.00000\n",
      "[2]\tval-logloss:0.81978\tval-recall:1.00000\n",
      "[3]\tval-logloss:0.81909\tval-recall:1.00000\n",
      "[4]\tval-logloss:0.81840\tval-recall:1.00000\n",
      "[5]\tval-logloss:0.81762\tval-recall:1.00000\n",
      "[6]\tval-logloss:0.81700\tval-recall:1.00000\n",
      "[7]\tval-logloss:0.81621\tval-recall:1.00000\n",
      "[8]\tval-logloss:0.81562\tval-recall:1.00000\n",
      "[9]\tval-logloss:0.81488\tval-recall:1.00000\n",
      "[10]\tval-logloss:0.81440\tval-recall:1.00000\n",
      "[11]\tval-logloss:0.81390\tval-recall:1.00000\n",
      "[12]\tval-logloss:0.81338\tval-recall:1.00000\n",
      "[13]\tval-logloss:0.81303\tval-recall:1.00000\n",
      "[14]\tval-logloss:0.81261\tval-recall:1.00000\n",
      "[15]\tval-logloss:0.81208\tval-recall:1.00000\n",
      "[16]\tval-logloss:0.81152\tval-recall:1.00000\n",
      "[17]\tval-logloss:0.81093\tval-recall:1.00000\n",
      "[18]\tval-logloss:0.81063\tval-recall:1.00000\n",
      "[19]\tval-logloss:0.81014\tval-recall:1.00000\n",
      "[20]\tval-logloss:0.80965\tval-recall:1.00000\n",
      "[21]\tval-logloss:0.80910\tval-recall:1.00000\n",
      "[22]\tval-logloss:0.80870\tval-recall:1.00000\n",
      "[23]\tval-logloss:0.80822\tval-recall:1.00000\n",
      "[24]\tval-logloss:0.80795\tval-recall:1.00000\n",
      "[25]\tval-logloss:0.80745\tval-recall:1.00000\n",
      "[26]\tval-logloss:0.80697\tval-recall:1.00000\n",
      "[27]\tval-logloss:0.80645\tval-recall:1.00000\n",
      "[28]\tval-logloss:0.80597\tval-recall:1.00000\n",
      "[29]\tval-logloss:0.80550\tval-recall:1.00000\n",
      "[30]\tval-logloss:0.80522\tval-recall:1.00000\n",
      "[31]\tval-logloss:0.80485\tval-recall:1.00000\n",
      "[32]\tval-logloss:0.80453\tval-recall:1.00000\n",
      "[33]\tval-logloss:0.80415\tval-recall:1.00000\n",
      "[34]\tval-logloss:0.80368\tval-recall:1.00000\n",
      "[35]\tval-logloss:0.80320\tval-recall:1.00000\n",
      "[36]\tval-logloss:0.80273\tval-recall:1.00000\n",
      "[37]\tval-logloss:0.80232\tval-recall:1.00000\n",
      "[38]\tval-logloss:0.80191\tval-recall:1.00000\n",
      "[39]\tval-logloss:0.80153\tval-recall:1.00000\n",
      "[40]\tval-logloss:0.80112\tval-recall:1.00000\n",
      "[41]\tval-logloss:0.80080\tval-recall:1.00000\n",
      "[42]\tval-logloss:0.80048\tval-recall:1.00000\n",
      "[43]\tval-logloss:0.80010\tval-recall:1.00000\n",
      "[44]\tval-logloss:0.79979\tval-recall:1.00000\n",
      "[45]\tval-logloss:0.79940\tval-recall:1.00000\n",
      "[46]\tval-logloss:0.79908\tval-recall:1.00000\n",
      "[47]\tval-logloss:0.79873\tval-recall:1.00000\n",
      "[48]\tval-logloss:0.79836\tval-recall:1.00000\n",
      "[49]\tval-logloss:0.79807\tval-recall:1.00000\n",
      "\n",
      "[Test Set Evaluation]\n",
      "AUC: 0.6664\n",
      "\n",
      "---- Threshold = 0.50 ----\n",
      "Precision: 0.3344, Recall: 1.0000, F1: 0.5012\n",
      "\n",
      "---- Threshold = 0.60 ----\n",
      "Precision: 0.3845, Recall: 0.8956, F1: 0.5381\n",
      "\n",
      "---- Threshold = 0.70 ----\n",
      "Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/oscar/home/ywan1077/.conda/envs/citizens/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "final_model, best_params = tune_xgb_native(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cv_model_thresholds(X, y, best_params, thresholds=[0.5], n_splits=5):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    for t in thresholds:\n",
    "        recalls, precisions, f1s, accs, aucs = [], [], [], [], []\n",
    "        print(f\"\\n========== Threshold = {t:.2f} ==========\")\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "            dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "            dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "            booster = xgb.train(\n",
    "                params={**best_params, \"objective\": \"binary:logistic\"},\n",
    "                dtrain=dtrain,\n",
    "                num_boost_round=1000,\n",
    "                evals=[(dval, \"val\")],\n",
    "                early_stopping_rounds=50,\n",
    "                verbose_eval=False\n",
    "            )\n",
    "\n",
    "            y_prob = booster.predict(dval)\n",
    "            y_pred = (y_prob > t).astype(int)\n",
    "\n",
    "            recalls.append(recall_score(y_val, y_pred))\n",
    "            precisions.append(precision_score(y_val, y_pred))\n",
    "            f1s.append(f1_score(y_val, y_pred))\n",
    "            accs.append(accuracy_score(y_val, y_pred))\n",
    "            aucs.append(roc_auc_score(y_val, y_prob))\n",
    "\n",
    "            print(f\"[Fold {fold}] Acc: {accs[-1]:.4f} | Recall: {recalls[-1]:.4f} | Precision: {precisions[-1]:.4f} | F1: {f1s[-1]:.4f} | AUC: {aucs[-1]:.4f}\")\n",
    "\n",
    "        print(f\"\\n[Threshold {t:.2f}] CV Avg Results:\")\n",
    "        print(f\"Accuracy : {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "        print(f\"Recall   : {np.mean(recalls):.4f} ± {np.std(recalls):.4f}\")\n",
    "        print(f\"Precision: {np.mean(precisions):.4f} ± {np.std(precisions):.4f}\")\n",
    "        print(f\"F1-score : {np.mean(f1s):.4f} ± {np.std(f1s):.4f}\")\n",
    "        print(f\"AUC      : {np.mean(aucs):.4f} ± {np.std(aucs):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Threshold = 0.50 ==========\n",
      "[Fold 1] Acc: 0.4826 | Recall: 0.9243 | Precision: 0.3859 | F1: 0.5444 | AUC: 0.6836\n",
      "[Fold 2] Acc: 0.4914 | Recall: 0.9138 | Precision: 0.3892 | F1: 0.5459 | AUC: 0.6974\n",
      "[Fold 3] Acc: 0.4979 | Recall: 0.9211 | Precision: 0.3932 | F1: 0.5511 | AUC: 0.6851\n",
      "[Fold 4] Acc: 0.4989 | Recall: 0.8991 | Precision: 0.3917 | F1: 0.5456 | AUC: 0.6790\n",
      "[Fold 5] Acc: 0.4986 | Recall: 0.8937 | Precision: 0.3907 | F1: 0.5437 | AUC: 0.6700\n",
      "\n",
      "[Threshold 0.50] CV Avg Results:\n",
      "Accuracy : 0.4939 ± 0.0063\n",
      "Recall   : 0.9104 ± 0.0121\n",
      "Precision: 0.3901 ± 0.0025\n",
      "F1-score : 0.5461 ± 0.0026\n",
      "AUC      : 0.6830 ± 0.0089\n",
      "\n",
      "========== Threshold = 0.60 ==========\n",
      "[Fold 1] Acc: 0.5790 | Recall: 0.7897 | Precision: 0.4296 | F1: 0.5565 | AUC: 0.6836\n",
      "[Fold 2] Acc: 0.5765 | Recall: 0.7781 | Precision: 0.4270 | F1: 0.5514 | AUC: 0.6974\n",
      "[Fold 3] Acc: 0.5788 | Recall: 0.7687 | Precision: 0.4280 | F1: 0.5498 | AUC: 0.6851\n",
      "[Fold 4] Acc: 0.5820 | Recall: 0.7403 | Precision: 0.4280 | F1: 0.5424 | AUC: 0.6790\n",
      "[Fold 5] Acc: 0.5778 | Recall: 0.7232 | Precision: 0.4230 | F1: 0.5338 | AUC: 0.6700\n",
      "\n",
      "[Threshold 0.60] CV Avg Results:\n",
      "Accuracy : 0.5788 ± 0.0018\n",
      "Recall   : 0.7600 ± 0.0246\n",
      "Precision: 0.4271 ± 0.0022\n",
      "F1-score : 0.5468 ± 0.0079\n",
      "AUC      : 0.6830 ± 0.0089\n",
      "\n",
      "========== Threshold = 0.70 ==========\n",
      "[Fold 1] Acc: 0.6613 | Recall: 0.4763 | Precision: 0.4935 | F1: 0.4848 | AUC: 0.6836\n",
      "[Fold 2] Acc: 0.6782 | Recall: 0.4890 | Precision: 0.5201 | F1: 0.5041 | AUC: 0.6974\n",
      "[Fold 3] Acc: 0.6643 | Recall: 0.4900 | Precision: 0.4984 | F1: 0.4942 | AUC: 0.6851\n",
      "[Fold 4] Acc: 0.6650 | Recall: 0.4679 | Precision: 0.4994 | F1: 0.4832 | AUC: 0.6790\n",
      "[Fold 5] Acc: 0.6510 | Recall: 0.4453 | Precision: 0.4764 | F1: 0.4603 | AUC: 0.6700\n",
      "\n",
      "[Threshold 0.70] CV Avg Results:\n",
      "Accuracy : 0.6639 ± 0.0087\n",
      "Recall   : 0.4737 ± 0.0164\n",
      "Precision: 0.4976 ± 0.0140\n",
      "F1-score : 0.4853 ± 0.0146\n",
      "AUC      : 0.6830 ± 0.0089\n"
     ]
    }
   ],
   "source": [
    "\n",
    "evaluate_cv_model_thresholds(\n",
    "    X_train, y_train,\n",
    "    best_params=\n",
    "{'alpha': 15, 'colsample_bytree': 0.5, 'gamma': 0.1, 'lambda': 1, 'learning_rate': 0.01, 'max_depth': 2, 'min_child_weight': 1, 'scale_pos_weight': 3.5, 'subsample': 0.5},\n",
    "    thresholds=[0.5, 0.6, 0.7]\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
